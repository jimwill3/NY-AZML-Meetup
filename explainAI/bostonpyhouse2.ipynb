{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.72  is the version of azureml sdk core\n",
      "--continue--\n",
      "--torch continue--\n",
      "1.2.0  torch version\n",
      "--interpret ok--\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml.contrib.interpret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c1017fbe9d9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--interpret ok--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExplanationDashboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--contrib ok--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'azureml.contrib.interpret'"
     ]
    }
   ],
   "source": [
    "#!pip install tensorwatch \n",
    "import azureml.core\n",
    "\n",
    "print(azureml.core.VERSION, ' is the version of azureml sdk core')\n",
    "import azureml.explain\n",
    "print('--continue--')\n",
    "\n",
    "import torch as T\n",
    "print('--torch continue--')\n",
    "print(T.__version__, ' torch version')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "print('--interpret ok--')\n",
    "\n",
    "#from azureml.contrib.interpret.visualize import ExplanationDashboard\n",
    "#print('--contrib ok--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_x, data_y, pct_close):\n",
    "  n_items = len(data_y)\n",
    "  X = T.Tensor(data_x)  # 2-d Tensor\n",
    "  Y = T.Tensor(data_y)  # actual as 1-d Tensor\n",
    "\n",
    "  oupt = model(X)       # all predicted as 2-d Tensor\n",
    "  pred = oupt.view(n_items)  # all predicted as 1-d\n",
    "\n",
    "  n_correct = T.sum((T.abs(pred - Y) < T.abs(pct_close * Y)))\n",
    "  result = (n_correct.item() * 100.0 / n_items)  # scalar\n",
    "  return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.hid1 = T.nn.Linear(13, 10)  # 13-(10-10)-1\n",
    "    self.hid2 = T.nn.Linear(10, 10)\n",
    "    self.oupt = T.nn.Linear(10, 1)\n",
    "\n",
    "    T.nn.init.xavier_uniform_(self.hid1.weight)  # glorot\n",
    "    T.nn.init.zeros_(self.hid1.bias)\n",
    "    T.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "    T.nn.init.zeros_(self.hid2.bias)\n",
    "    T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "    T.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = T.tanh(self.hid1(x))  # or T.nn.Tanh()\n",
    "    z = T.tanh(self.hid2(z))\n",
    "    z = self.oupt(z)  # no activation, aka Identity()\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "  # 0. get started\n",
    "  print(\"\\nBoston regression using PyTorch 1.0 \\n\")\n",
    "  T.manual_seed(1);  np.random.seed(1)\n",
    "\n",
    "  # 1. load data\n",
    "  print(\"Loading Boston data into memory \")\n",
    "  train_file = \"boston_train.txt\"\n",
    "  test_file = \"boston_test.txt\"\n",
    "\n",
    "  train_x = np.loadtxt(train_file, delimiter=\"\\t\",\n",
    "    usecols=range(0,13), dtype=np.float32)\n",
    "  train_y = np.loadtxt(train_file, delimiter=\"\\t\",\n",
    "    usecols=[13], dtype=np.float32)\n",
    "  test_x = np.loadtxt(test_file, delimiter=\"\\t\",\n",
    "    usecols=range(0,13), dtype=np.float32)\n",
    "  test_y = np.loadtxt(test_file, delimiter=\"\\t\",\n",
    "    usecols=[13], dtype=np.float32)\n",
    "\n",
    "  #look at our training data\n",
    "  print((train_x))\n",
    "\n",
    "  # 2. create model\n",
    "  print(\"Creating 13-(10-10)-1 DNN regression model \\n\")\n",
    "  net = Net()  # all work done above\n",
    "\n",
    "  # 3. train model\n",
    "  net = net.train()  # set training mode\n",
    "  bat_size = 30 #was 10\n",
    "  loss_func = T.nn.MSELoss()  # mean squared error\n",
    "  optimizer = T.optim.SGD(net.parameters(), lr=0.01)\n",
    "  n_items = len(train_x)\n",
    "  batches_per_epoch = n_items // bat_size\n",
    "  max_batches = 1000 * batches_per_epoch\n",
    "\n",
    "  print(\"Starting training\")\n",
    "  for b in range(max_batches):\n",
    "    curr_bat = np.random.choice(n_items, bat_size,\n",
    "      replace=False)\n",
    "    X = T.Tensor(train_x[curr_bat])\n",
    "    Y = T.Tensor(train_y[curr_bat]).view(bat_size,1)\n",
    "    optimizer.zero_grad()\n",
    "    oupt = net(X)\n",
    "      \n",
    "    loss_obj = loss_func(oupt, Y)\n",
    "    loss_obj.backward()  # compute gradients\n",
    "    optimizer.step()     # update weights and biases\n",
    "\n",
    "    if b % (max_batches // 10) == 0:\n",
    "      print(\"batch = %6d\" % b, end=\"\")\n",
    "      print(\"  batch loss = %7.4f\" % loss_obj.item(), end=\"\")\n",
    "      net = net.eval()\n",
    "      acc = accuracy(net, train_x, train_y, 0.15) \n",
    "      net = net.train()\n",
    "      print(\"  accuracy = %0.2f%%\" % acc)       \n",
    "  print(\"Training complete \\n\")\n",
    "\n",
    "  # 4. evaluate model\n",
    "  net = net.eval()  # set eval mode\n",
    "  acc = accuracy(net, test_x, test_y, 0.15) \n",
    "  print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
    "  raw_inpt = np.array([[0.09266, 34, 6.09, 0, 0.433, 6.495, 18.4,\n",
    "    5.4917, 7, 329, 16.1, 383.61, 8.67]], dtype=np.float32)\n",
    "\n",
    "  norm_inpt = np.array([[0.000970, 0.340000, 0.198148, -1,\n",
    "    0.098765, 0.562177, 0.159629, 0.396666, 0.260870, 0.270992,\n",
    "    0.372340, 0.966488, 0.191501]], dtype=np.float32)\n",
    "\n",
    "  y = net(X)\n",
    "  X = T.Tensor(norm_inpt)\n",
    "  \n",
    "  print(repr(y.item))\n",
    "  print(y[0])\n",
    "  print(\"For a town with raw input values: \")\n",
    "  for (idx,val) in enumerate(raw_inpt[0]):\n",
    "    if idx % 5 == 0: print(\"\")\n",
    "    print(\"%11.6f \" % val, end=\"\")\n",
    "    \n",
    "    print(\"\\n\\nPredicted median house price = $%0.2f\" % (y[idx]*10000))\n",
    "  \n",
    "\n",
    "  explainer = TabularExplainer(net, train_x) #, \n",
    "                             #features = boston_data.feature_names)\n",
    "  global_explanation = explainer.explain_global(test_x)\n",
    "  # Sorted SHAP values \n",
    "  print('ranked global importance values: {}'.format(global_explanation.get_ranked_global_values()))\n",
    "  # Corresponding feature names\n",
    "  print('ranked global importance names: {}'.format(global_explanation.get_ranked_global_names()))\n",
    "  # Feature ranks (based on original order of features)\n",
    "  print('global importance rank: {}'.format(global_explanation.global_importance_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Boston regression using PyTorch 1.0 \n",
      "\n",
      "Loading Boston data into memory \n",
      "[[4.83000e-04 3.30000e-01 5.33330e-02 ... 6.17021e-01 1.00000e+00\n",
      "  1.60044e-01]\n",
      " [2.15000e-04 5.50000e-01 1.12593e-01 ... 5.31915e-01 1.00000e+00\n",
      "  1.50386e-01]\n",
      " [2.50600e-03 0.00000e+00 2.28519e-01 ... 5.63830e-01 9.89510e-01\n",
      "  4.71026e-01]\n",
      " ...\n",
      " [1.05400e-03 0.00000e+00 6.37040e-02 ... 5.53191e-01 1.00000e+00\n",
      "  1.08996e-01]\n",
      " [1.67470e-02 0.00000e+00 6.97778e-01 ... 2.23404e-01 8.60558e-01\n",
      "  3.18709e-01]\n",
      " [8.70620e-02 0.00000e+00 6.42963e-01 ... 8.08511e-01 6.85587e-01\n",
      "  4.00110e-01]]\n",
      "Creating 13-(10-10)-1 DNN regression model \n",
      "\n",
      "Starting training\n",
      "batch =      0  batch loss =  4.7272  accuracy = 2.97%\n",
      "batch =   1300  batch loss =  0.0898  accuracy = 69.06%\n",
      "batch =   2600  batch loss =  0.1110  accuracy = 71.53%\n",
      "batch =   3900  batch loss =  0.0694  accuracy = 72.28%\n",
      "batch =   5200  batch loss =  0.1737  accuracy = 72.77%\n",
      "batch =   6500  batch loss =  0.0918  accuracy = 73.51%\n",
      "batch =   7800  batch loss =  0.1333  accuracy = 75.50%\n",
      "batch =   9100  batch loss =  0.0581  accuracy = 75.50%\n",
      "batch =  10400  batch loss =  0.1661  accuracy = 71.04%\n",
      "batch =  11700  batch loss =  0.0740  accuracy = 75.50%\n",
      "Training complete \n",
      "\n",
      "Accuracy on test data = 69.61%\n",
      "<built-in method item of Tensor object at 0x000001ADA607C9D8>\n",
      "tensor([2.7893], grad_fn=<SelectBackward>)\n",
      "For a town with raw input values: \n",
      "\n",
      "   0.092660 \n",
      "\n",
      "Predicted median house price = $27893.16\n",
      "  34.000000 \n",
      "\n",
      "Predicted median house price = $26248.17\n",
      "   6.090000 \n",
      "\n",
      "Predicted median house price = $15626.91\n",
      "   0.000000 \n",
      "\n",
      "Predicted median house price = $30731.57\n",
      "   0.433000 \n",
      "\n",
      "Predicted median house price = $19588.53\n",
      "\n",
      "   6.495000 \n",
      "\n",
      "Predicted median house price = $14635.40\n",
      "  18.400000 \n",
      "\n",
      "Predicted median house price = $10957.08\n",
      "   5.491700 \n",
      "\n",
      "Predicted median house price = $33025.77\n",
      "   7.000000 \n",
      "\n",
      "Predicted median house price = $9261.56\n",
      " 329.000000 \n",
      "\n",
      "Predicted median house price = $19222.32\n",
      "\n",
      "  16.100000 \n",
      "\n",
      "Predicted median house price = $20342.93\n",
      " 383.609985 \n",
      "\n",
      "Predicted median house price = $34215.13\n",
      "   8.670000 \n",
      "\n",
      "Predicted median house price = $24645.86\n",
      "ranked global importance values: [0.4723749725403739, 0.30503038609144734, 0.1755203196103228, 0.16412065449106855, 0.15853797764900854, 0.13440133681904307, 0.12217150877147694, 0.08968860115490708, 0.06672648242423676, 0.03475908899460645, 0.03018485567550741, 0.026842279688400382, 0.022921871635309586]\n",
      "ranked global importance names: [12, 8, 4, 7, 3, 5, 9, 10, 0, 11, 6, 1, 2]\n",
      "global importance rank: [12, 8, 4, 7, 3, 5, 9, 10, 0, 11, 6, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "#net=Net()\n",
    "main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml.contrib.interpret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f63c476fbbf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExplanationDashboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'azureml.contrib.interpret'"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.interpret.visualize import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, net, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
