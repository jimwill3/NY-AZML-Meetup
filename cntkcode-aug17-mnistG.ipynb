{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an \"almost\" mirror, with small changes to cntkcode-aug17-mnistF - named: cntkcode-aug17-mnistG\n",
    "The main difference is use of the graph based API to define and create the convolutional network and a chance to \n",
    "explore the runtime errors you can get if you make a small mistake in that more verbose definition.\n",
    "There are also some different tracing and debugging statements in here as cramming all of that into one notebook \n",
    "would make that notebook quite busy. As in mnistF - some local directory paths will need to be altered by you and keep in mind that all the data and general flow is based on the CNTK tutorial 103D and its prerequisites.\n",
    "Note that there are VS code versions of these that will be posted to github since VS code supports an integrated python terminal option to run the code - which supports using the cntk runtime debugger. if you can get a jupyter notebook to expose a terminal (i've been unable to on both windows and linux), you should be able to stay in the notebook to run it. This also plays with where the dropout layer is defined...should it be before of after the dense layer of 96 nodes ... do some testing and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNTK version 2.0\n",
      "CNTK sees device(s):  (CPU,)\n",
      "Redirecting log to file mnistG.log\n",
      "Data directory is C:\\local\\cntk-2-0\\cntk\\Scripts\\CNTK-Samples-2-0\\Examples\\Image\\DataSets\\MNIST\n",
      "CNTK Objects/Functions Dump\n",
      "name\t Classifier Details:Dense: Output('Dense96', [#], [96]) -> Output('Classifier', [#], [10]):<class 'cntk.ops.functions.Function'>\n",
      "name\t Dense96 Details:Dense: Output('DropOutB4Dense96', [#], [16 x 4 x 4]) -> Output('Dense96', [#], [96]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Plus: Output('Times3428_Output_0', [#], [10]) -> Output('Plus3430_Output_0', [#], [10]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Times: Placeholder('Dense96', [#], [96]) -> Output('Times3428_Output_0', [#], [10]):<class 'cntk.ops.functions.Function'>\n",
      "name\t W Details:Parameter('W', [], [96 x 10]):<class 'cntk.variables.Parameter'>\n",
      "name\t b Details:Parameter('b', [], [10]):<class 'cntk.variables.Parameter'>\n",
      "name\t DropOutB4Dense96 Details:Dropout: Output('Second_Max', [#], [16 x 4 x 4]) -> Output('DropOutB4Dense96', [#], [16 x 4 x 4]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:ReLU: Output('Plus3358_Output_0', [#], [96]) -> Output('ReLU3360_Output_0', [#], [96]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Plus: Output('Times3356_Output_0', [#], [96]) -> Output('Plus3358_Output_0', [#], [96]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Times: Placeholder('DropOutB4Dense96', [#], [16 x 4 x 4]) -> Output('Times3356_Output_0', [#], [96]):<class 'cntk.ops.functions.Function'>\n",
      "name\t W Details:Parameter('W', [], [16 x 4 x 4 x 96]):<class 'cntk.variables.Parameter'>\n",
      "name\t b Details:Parameter('b', [], [96]):<class 'cntk.variables.Parameter'>\n",
      "name\t Second_Max Details:MaxPooling: Output('Second_Conv', [#], [16 x 14 x 14]) -> Output('Second_Max', [#], [16 x 4 x 4]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Dropout: Placeholder('Second_Max', [#], [16 x 4 x 4]) -> Output('Dropout3296_Output_0', [#], [16 x 4 x 4]):<class 'cntk.ops.functions.Function'>\n",
      "name\t Second_Conv Details:Convolution: Output('First_Max', [#], [8 x 14 x 14]) -> Output('Second_Conv', [#], [16 x 14 x 14]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Pooling: Placeholder('Second_Conv', [#], [16 x 14 x 14]) -> Output('Pooling3250_Output_0', [#], [16 x 4 x 4]):<class 'cntk.ops.functions.Function'>\n",
      "name\t First_Max Details:MaxPooling: Output('First_Conv', [#], [8 x 28 x 28]) -> Output('First_Max', [#], [8 x 14 x 14]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:ReLU: Output('Plus3202_Output_0', [#], [16 x 14 x 14]) -> Output('ReLU3204_Output_0', [#], [16 x 14 x 14]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Plus: Output('Convolution3200_Output_0', [#], [16 x 14 x 14]) -> Output('Plus3202_Output_0', [#], [16 x 14 x 14]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Convolution: Placeholder('First_Max', [#], [8 x 14 x 14]) -> Output('Convolution3200_Output_0', [#], [16 x 14 x 14]):<class 'cntk.ops.functions.Function'>\n",
      "name\t W Details:Parameter('W', [], [16 x 8 x 3 x 3]):<class 'cntk.variables.Parameter'>\n",
      "name\t b Details:Parameter('b', [], [16 x 1 x 1]):<class 'cntk.variables.Parameter'>\n",
      "name\t First_Conv Details:Convolution: Input('Input3093', [#], [1 x 28 x 28]) -> Output('First_Conv', [#], [8 x 28 x 28]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Pooling: Placeholder('First_Conv', [#], [8 x 28 x 28]) -> Output('Pooling3156_Output_0', [#], [8 x 14 x 14]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Input('Input3093', [#], [1 x 28 x 28]):<class 'cntk.variables.Variable'>\n",
      "name\t  Details:ReLU: Output('Plus3120_Output_0', [#], [8 x 28 x 28]) -> Output('ReLU3122_Output_0', [#], [8 x 28 x 28]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Plus: Output('Convolution3118_Output_0', [#], [8 x 28 x 28]) -> Output('Plus3120_Output_0', [#], [8 x 28 x 28]):<class 'cntk.ops.functions.Function'>\n",
      "name\t  Details:Convolution: Placeholder('Placeholder3455', [#], [1 x 28 x 28]) -> Output('Convolution3118_Output_0', [#], [8 x 28 x 28]):<class 'cntk.ops.functions.Function'>\n",
      "name\t W Details:Parameter('W', [], [8 x 1 x 5 x 5]):<class 'cntk.variables.Parameter'>\n",
      "name\t b Details:Parameter('b', [], [8 x 1 x 1]):<class 'cntk.variables.Parameter'>\n",
      "---------------------------------------------------------------------\n",
      "final model:\n",
      "  Composite(Dense): Input('Input3093', [#], [1 x 28 x 28]) -> Output('Classifier', [#], [10])\n",
      "the z network model is  Composite(Dense): Input('Input3093', [#], [1 x 28 x 28]) -> Output('Classifier', [#], [10])\n",
      "model type is:  <class 'cntk.ops.functions.Function'>\n",
      "Minibatch: 0, Loss: 2.3083, Error: 85.94%\n",
      "Minibatch: 500, Loss: 0.2618, Error: 12.50%\n",
      "Minibatch: 1000, Loss: 0.1333, Error: 1.56%\n",
      "Minibatch: 1500, Loss: 0.2127, Error: 7.81%\n",
      "Minibatch: 2000, Loss: 0.0937, Error: 3.12%\n",
      "Minibatch: 2500, Loss: 0.0265, Error: 0.00%\n",
      "Minibatch: 3000, Loss: 0.0309, Error: 0.00%\n",
      "Minibatch: 3500, Loss: 0.1844, Error: 6.25%\n",
      "Minibatch: 4000, Loss: 0.0421, Error: 3.12%\n",
      "Minibatch: 4500, Loss: 0.0726, Error: 3.12%\n",
      "Minibatch: 5000, Loss: 0.0501, Error: 1.56%\n",
      "Minibatch: 5500, Loss: 0.0267, Error: 1.56%\n",
      "Minibatch: 6000, Loss: 0.1419, Error: 1.56%\n",
      "Minibatch: 6500, Loss: 0.0803, Error: 3.12%\n",
      "Minibatch: 7000, Loss: 0.1122, Error: 4.69%\n",
      "Minibatch: 7500, Loss: 0.0744, Error: 3.12%\n",
      "Minibatch: 8000, Loss: 0.0483, Error: 1.56%\n",
      "Minibatch: 8500, Loss: 0.0142, Error: 0.00%\n",
      "Minibatch: 9000, Loss: 0.0623, Error: 3.12%\n",
      "Training took 377.9 sec\n",
      "Average test error: 1.05%\n"
     ]
    }
   ],
   "source": [
    "#mnistG.py - graph API usage for a conv net with cntk\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cntk as C\n",
    "import datetime\n",
    "\n",
    "print(\"CNTK version\",C.__version__)\n",
    "#let's set up safety and tracing flags\n",
    "C.debugging.set_computation_network_trace_level(1) #1000 is more and 1000000 is intense\n",
    "\n",
    "print(\"CNTK sees device(s): \",C.all_devices())\n",
    "#Ensure we always get the same randomizer init\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the data dimensions\n",
    "input_dim_model = (1, 28, 28)    # images are 28 x 28 with 1 channel of color (gray)\n",
    "input_dim = 28*28                # used by readers to treat input data as a vector\n",
    "num_output_classes = 10\n",
    "\n",
    "#we need a real progress printer\n",
    "progprint = C.logging.progress_print.ProgressPrinter(freq=0,first=2,log_to_file='mnistG.log')\n",
    "progprint.log('Initializng the Model ' + repr(datetime.datetime.now().strftime(\"%A, %d. %B %Y %I:%M%p\")))\n",
    "\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    \n",
    "    ctf = C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "          labels=C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "          features=C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)))\n",
    "                          \n",
    "    return C.io.MinibatchSource(ctf,\n",
    "        randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "data_found=False # A flag to indicate if train/test data found in local cache\n",
    "for data_dir in [os.path.join(\"C:\\local\\cntk-2-0\\cntk\\Scripts\\CNTK-Samples-2-0\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                 os.path.join(\"data\", \"MNIST\")]:\n",
    "    \n",
    "    train_file=os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file=os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    \n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found=True\n",
    "        break\n",
    "        \n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing CNTK 103 Part A\")\n",
    "    \n",
    "print(\"Data directory is {0}\".format(data_dir))\n",
    "\n",
    "#prep our input/output containers/shapes\n",
    "x = C.input_variable(input_dim_model)\n",
    "y = C.input_variable(num_output_classes)\n",
    "\n",
    "def create_criterion_function(model, labels):\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error(model, labels)\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)\n",
    "\n",
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error\n",
    "\n",
    "\n",
    "def train_test(train_reader, test_reader, model_func, num_sweeps_to_train_with=10):  #sweeps was 10\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    # We will scale the input image pixels within 0-1 range by dividing all input value by 255.\n",
    "    model = model_func(x/255)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, y)\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.2\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(z.parameters, lr_schedule)\n",
    "    trainer = C.Trainer(z, (loss, label_error), [learner],[progprint]) #bind up trainer, learner,model,loss,printer\n",
    "    \n",
    "    \n",
    "    # Initialize the parameters for the trainer\n",
    "    minibatch_size = 64           #was 64\n",
    "    num_samples_per_sweep = 60000 #was 60000 - use 6000 for demos\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    # Map the data streams to the input and labels.\n",
    "    input_map={\n",
    "        y  : train_reader.streams.labels,\n",
    "        x  : train_reader.streams.features\n",
    "    } \n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    training_progress_output_freq = 500\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        # Read a mini batch from the training data file\n",
    "        data=train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        trainer.train_minibatch(data)\n",
    "        print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "    # Print training time\n",
    "    print(\"Training took {:.1f} sec\".format(time.time() - start))\n",
    "    progprint.log('Training done - logging model prior to testing')\n",
    "    \n",
    "    cntkfuncs = C.logging.graph.depth_first_search(trainer.model,lambda j: (True),depth=-1)\n",
    "    for afunc in cntkfuncs:\n",
    "        progprint.log(\"Name: \"+ afunc.name + \" Val: \" + repr(afunc))    # + \"Type: \"+ str(type(afunc)))\n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        y  : test_reader.streams.labels,\n",
    "        x  : test_reader.streams.features\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 512 #was 512\n",
    "    num_samples = 10000 #was 10000 - make it 1000 for demos \n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "\n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))\n",
    "\n",
    "    #add do traintest\n",
    "def do_train_test():\n",
    "        global z\n",
    "        z = create_model(x)\n",
    "        print(\"the z network model is \",repr(z))\n",
    "        print(\"model type is: \",type(z))\n",
    "        reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "        reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "        train_test(reader_train, reader_test, z)\n",
    "\n",
    "\n",
    "# function to build model graph API approach\n",
    "def create_model(features):\n",
    "    #the defaults can be overridden at layer level\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.relu):\n",
    "            h = features\n",
    "            \n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5),  #was 5,5\n",
    "                                       num_filters=8,       #was 8 \n",
    "                                       strides=(1,1),\n",
    "                                       #activation=C.sigmoid, #jw testing 8/15 - high error rate 88%\n",
    "                                       pad=True, name=\"First_Conv\")(h)\n",
    "            #print(\"first conv layer: \",h)\n",
    "            h = C.layers.MaxPooling(filter_shape=(2,2), \n",
    "                                    strides=(2,2), name=\"First_Max\")(h)\n",
    "            #print(\"max_pool:\",repr(h))\n",
    "            h = C.layers.Convolution2D(filter_shape=(3,3),  #was 5,5\n",
    "                                       num_filters=16,      #was 16\n",
    "                                       strides=(1,1),       #was (1,1)\n",
    "                                       pad=True, name=\"Second_Conv\")(h) #comment out the (h) and run this \n",
    "            #print(\"second conv layer:\",repr(h))\n",
    "            #h = C.layers.Convolution2D(filter_shape=(5,5),\n",
    "            #                          num_filters=24,\n",
    "            #                          strides=(1,1),\n",
    "            #                          pad=True,name=\"third_conv\") (h)\n",
    "            #print(\"third conv layer\",repr(h))\n",
    "            #this third layer above was added 8/9 2:25PM for testing only\n",
    "            h = C.layers.MaxPooling(filter_shape=(3,3), \n",
    "                                    strides=(3,3), name=\"Second_Max\") (h)\n",
    "            r = C.layers.Dropout(0.5,name='DropOutB4Dense96')(h) #tests at 0.88 error\n",
    "            #add a dense layer for testing - 8/14 - quiz should the droput be defined before or after the dense layer?\n",
    "            r = C.layers.Dense(96,name='Dense96')(r)    # add a layer of 96 densely connection nodes\n",
    "            #r = C.layers.layers.Dropout(0.5,name='DropOutAfterDense96')(r)   # error is 1.08...\n",
    "            r = C.layers.Dense(num_output_classes,activation=None,name='Classifier')(r)\n",
    "            #r = C.layers.layers.Dropout(0.5,name='DropOutAfterDenseClassifier')(r) #jw 8/14 0.39 avg test error\n",
    "            #print(\"\\n final network\",repr(r)) #add 8/8 1:00PM - shows droput\n",
    "            cntkfunclist = C.logging.graph.depth_first_search(r,lambda j: (True),depth=-1)\n",
    "            print(\"CNTK Objects/Functions Dump\")\n",
    "            for afunc in cntkfunclist:\n",
    "                print(\"name\\t\",afunc.name +\" Details:\" + repr(afunc)+ \":\"+ str(type(afunc)))\n",
    "            #get the weights\n",
    "            progprint.log(\"FirstConv Weight: \"+ repr(r.First_Conv.W.value)) #show how to get a layers W values\n",
    "            #print(\"start:\\t\",repr(C.logging.find_by_name(r,'first_conv',depth=-1)))\n",
    "            #firstconvfunc= C.logging.find_by_name(r,'first_conv',depth=-1)\n",
    "            #print(\"first conv outputs:\\t\",firstconvfunc.outputs)\n",
    "            #print(\"first conv block arg map:\\t\",firstconvfunc.block_arguments_mapping)\n",
    "            print('---------------------------------------------------------------------')\n",
    "            #print(\"next:\\t\",repr(C.logging.find_by_name(r,'first_max',depth=-1)))\n",
    "            #print(\"next:\\t\",repr(C.logging.find_by_name(r,'second_conv',depth=-1)))\n",
    "            #print(\"next:\\t\",repr(C.logging.find_by_name(r,'second_max',depth=-1)))\n",
    "            #print(\"next:\\t\",repr(C.logging.find_by_name(r,'96',depth=-1)))\n",
    "            #print('----------------------------------------------------------------------')\n",
    "            \n",
    "            print(\"final model:\\n \",repr(r))            \n",
    "            #r=C.debugging.debug_model(r) - not good wo terminal\n",
    "            return r\n",
    "        \n",
    "do_train_test()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
