{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "june 29 cntk session azure ml meetup - insprired by dr james mccaffrey article in jan 2017 msdn on CNTK\n",
    "this is a python version of the small problem originally done in brainscript\n",
    "any bugs or issues here are the author's - not dr mccaffrey's\n",
    "jimw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cntk as C\n",
    " \n",
    "from   cntk.learners import sgd, learning_rate_schedule, UnitType\n",
    "from   cntk.logging import ProgressPrinter        # kind of a logger\n",
    "from   cntk.layers import Dense, Sequential       #preworked layers we can choose to use (or ignore)\n",
    "from   cntk.io import CTFDeserializer, MinibatchSource,StreamDef,StreamDefs\n",
    "from   cntk.io import INFINITELY_REPEAT \n",
    "from   cntk.train.training_session import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_dim    = 2     #2 features to input per sample\n",
    "output_dim   = 3     #3 classes/labels each sample\n",
    "layers_dim   = 1     #1 hidden dimension as this is just for starting\n",
    "hidden2_dim  = 5     #5 hidden nodes in the 1 hidden dimension\n",
    "\n",
    "input_var    = C.input(input_dim,np.float32)\n",
    "label_var    = C.input(output_dim,np.float32)\n",
    "inputfile    = \"C:\\\\Users\\\\jiwillia\\\\Documents\\\\meetup\\\\deepL\\\\CNTK-session2\\\\TrainData.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample of a section of the training file (traindata.txt) can also be used as-is for BrainScript\n",
    "|features 3.0 1.0 |labels 1 0 0\n",
    "|features 5.0 2.0 |labels 1 0 0\n",
    "|features 6.0 7.0 |labels 1 0 0\n",
    "|features 7.0 4.0 |labels 1 0 0\n",
    "|features 3.0 5.0 |labels 0 1 0\n",
    "|features 4.0 4.0 |labels 0 1 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reader function - returns a minibatch reader to use in training and testing of a network and learner\n",
    "def create_reader(pathtofile,is_training,inputsdim,outputsdim):\n",
    "    return MinibatchSource(CTFDeserializer(pathtofile,StreamDefs(labels=StreamDef(field='labels',shape=outputsdim,\n",
    "            is_sparse=False),\n",
    "            features=StreamDef(field='features',shape=inputsdim,is_sparse=False))),randomize=is_training,\n",
    "            max_sweeps=INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the minibatch reader and mapping to the features and labels that will be mapped to file content\n",
    "\n",
    "areader = create_reader(inputfile,True,input_dim,output_dim)\n",
    "\n",
    "#map input containers (tensors) with the text file structure\n",
    "reader_map = {\n",
    "    input_var : areader.streams.features,\n",
    "    label_var : areader.streams.labels\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per minibatch: 0.04\n",
      "    0.699      0.699          0          0             1\n",
      "    0.929       1.04      0.333        0.5             3\n",
      "     1.07       1.18      0.429        0.5             7\n",
      "     1.03          1        0.4      0.375            15\n",
      "    0.967      0.905      0.355      0.312            31\n",
      "     0.93      0.894      0.349      0.344            63\n",
      "    0.945      0.959      0.386      0.422           127\n",
      "    0.892       0.84      0.373      0.359           255\n",
      "    0.814      0.736      0.364      0.355           511\n",
      "    0.711      0.607      0.305      0.246          1023\n",
      "      0.6       0.49      0.279      0.253          2047\n",
      "    0.489      0.378       0.24      0.202          4095\n",
      "    0.385       0.28      0.172      0.104          8191\n",
      "    0.301      0.216      0.128     0.0835         16383\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "#let define a network and learner  - note how inputs are not included directly in the initial definition 'amodel'\n",
    "#mbs, minibatch schedule, is ignored for this one as the data is so small, may be useful in other situations\n",
    "amodel     = Sequential ([Dense(hidden2_dim,activation=C.sigmoid),Dense(output_dim)])\n",
    "zz         = amodel(input_var) \n",
    "mbsize     = 1      #original brainscript was 1\n",
    "numbatches = 18000  #same as the brainscript (500)*25 per batch\n",
    "crossent   = C.cross_entropy_with_softmax(zz,label_var)\n",
    "classerror = C.classification_error(zz,label_var)\n",
    "learn_rate = learning_rate_schedule(0.04,UnitType.minibatch) #.04 used in brainscript\n",
    "prprint    = ProgressPrinter(0)                              #0 means a geometric print schedule\n",
    "mbs        = minibatch_size_schedule([1,2],3000)             #use 1 for the first 1000 samples and then 2 after that\n",
    "\n",
    "#print(\"Model: \",amodel, \" Inputs: \",amodel.inputs) #just to see if the network looks like our 2 in, 5 hidden etc\n",
    "Trainer2   = C.Trainer(zz,(crossent,classerror), [sgd(zz.parameters,lr=learn_rate)],[prprint])\n",
    "\n",
    "agg_loss   = 0.0    #need to add this, testing, and prediction\n",
    "\n",
    "for ii in range(numbatches):\n",
    "    mbatch = areader.next_minibatch(mbsize,input_map=reader_map)\n",
    "    Trainer2.train_minibatch(mbatch)\n",
    "print('training done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with training done we can know try our test data file - same structure as training data just fresh content\n",
    "testdata.txt contains the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.11111111111111  error percent for  9 test samples\n"
     ]
    }
   ],
   "source": [
    "#load and process test data - use different approach since CNTK offers so many ways\n",
    "testinput =  \"c:\\\\Users\\\\jiwillia\\\\Documents\\\\meetup\\\\deepL\\\\CNTK-session2\\\\testdata.txt\"\n",
    "#sample entry from file:    |features 1.0 1.0 |labels 1 0 0\n",
    "\n",
    "testreader = create_reader(testinput,False,input_dim,output_dim)\n",
    "\n",
    "test_mb_size = 1 \n",
    "sample_size  = 9\n",
    "mb_to_test   = sample_size\n",
    "test_results = 0.0 \n",
    "for tt in range(sample_size):\n",
    "    mbtest = testreader.next_minibatch(test_mb_size,input_map=reader_map)\n",
    "    #print(mbtest)\n",
    "    eval_error = Trainer2.test_minibatch(mbtest)\n",
    "    test_results += eval_error\n",
    "    #print(\"EE : \",eval_error)\n",
    "    \n",
    "print((test_results/sample_size) * 100, \" error percent for \",sample_size, \"test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted array :  [[ 0.22659075 -4.40430784  4.7554059 ]]\n",
      "softmax of predicted is:  2  :as %  [[  1.06770927e-02   1.04058927e-04   9.89218891e-01]]\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "#let's try to predict skipping the testing phase for now\n",
    "unknown = np.array([[[9.0,1.0]]],dtype=np.float32) #equivalent to features 4.0, 7.0 labels -1,-1,-1 if this was in a file\n",
    "\n",
    "prediction = zz.eval({input_var : unknown})\n",
    "\n",
    "print(\"predicted array : \",prediction)\n",
    "print(\"softmax of predicted is: \",np.argmax(prediction),\" :as % \",C.softmax(prediction).eval())\n",
    "print('done...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
